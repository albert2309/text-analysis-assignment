{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1444059\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from urllib.request import urlopen\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-h/2554-h.htm\"\n",
    "sampleText = urlopen(url).read().decode('utf8')\n",
    "print(len(sampleText))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    a   e   i   o   u \n",
      "k 418 148  94 420 173 \n",
      "p  83  31 105  34  51 \n",
      "r 187  63  84  89  79 \n",
      "s   0   0 100   2   1 \n",
      "t  47   8   0 148  37 \n",
      "v  93  27 105  48  49 \n",
      "None\n",
      "    a   e   i   o   u \n",
      "k 418 148  94 420 173 \n",
      "p  83  31 105  34  51 \n",
      "r 187  63  84  89  79 \n",
      "s   0   0 100   2   1 \n",
      "t  47   8   0 148  37 \n",
      "v  93  27 105  48  49 \n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "rotokas_words = nltk.corpus.toolbox.words('rotokas.dic')\n",
    "cvs = [cv for w in rotokas_words for cv in re.findall(r'[ptksvr][aeiou]',w)]\n",
    "cfd = nltk.ConditionalFreqDist(cvs)\n",
    "print(cfd.tabulate())\n",
    "#The above statemetn can be interpreted as:\n",
    "test=[]\n",
    "for w in rotokas_words:\n",
    "    for cv in re.findall(r'[ptksvr][aeiou]',w):\n",
    "        test.append(cv)\n",
    "test2 = nltk.ConditionalFreqDist(test) \n",
    "print(test2.tabulate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.util.Index'>\n",
      "<class 'list'>\n",
      "['kasuari']\n",
      "['kaapo', 'kaapopato', 'kaipori', 'kaiporipie', 'kaiporivira', 'kapo', 'kapoa', 'kapokao', 'kapokapo', 'kapokapo', 'kapokapoa', 'kapokapoa', 'kapokapora', 'kapokapora', 'kapokaporo', 'kapokaporo', 'kapokari', 'kapokarito', 'kapokoa', 'kapoo', 'kapooto', 'kapoovira', 'kapopaa', 'kaporo', 'kaporo', 'kaporopa', 'kaporoto', 'kapoto', 'karokaropo', 'karopo', 'kepo', 'kepoi', 'keposi', 'kepoto']\n"
     ]
    }
   ],
   "source": [
    "cv_word_pairs = [(cv,w) for w in rotokas_words for cv in re.findall(r'[ptksvr][aeiou]',w)]\n",
    "cv_index = nltk.Index(cv_word_pairs)\n",
    "#Similar to JSON when printed\n",
    "print(type(cv_index))\n",
    "#JSON\n",
    "print(type(['foo', {'bar': ('baz', None, 1.0, 2)}]))\n",
    "print(cv_index['su'])\n",
    "print(cv_index['po'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resent\n"
     ]
    }
   ],
   "source": [
    "def stem(word):\n",
    "    for suffix in ['ing','ly','ed','ious','ies','ive','es','e','ment']:\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "    return word\n",
    "    \n",
    "print(stem('resentment'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the word for stemming: language\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('language', '')]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "#^. starts with any characters\n",
    "#*(ing|ly|ed|ious|ies|ive|es|s|ment)$ ends with any of this\n",
    "#The function checks if the word inputted has a suffix\n",
    "inputWord = input('Enter the word for stemming: ')\n",
    "print(re.findall(r'^.*(ing|ly|ed|ious|ies|ive|es|s|ment)$',inputWord))\n",
    "print(re.findall(r'^.*(?:ing|ly|ed|ious|ies|ive|es|s|ment)$',inputWord))\n",
    "print(re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$',inputWord))\n",
    "print(re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)$',inputWord))\n",
    "print(re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$',inputWord))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', 'probab', 'work', 'out', 'that', 'a', 'backlash', 'mean', 'that', 'the', 'follow', 'character', 'i', 'depriv', 'of', 'it', 'special', 'power', 'and', 'must', 'literal', 'match', 'a', 'specific', 'character', 'in', 'they', 'word', '.', 'Thu', ',', 'while', '.', 'i', 'special', ',', '\\\\', '.', 'on', 'match', 'a', 'period', '.', 'The', 'brac', 'expression', 'like', '{', '3,5', '}', ',', 'specify', 'the', 'number', 'of', 'repeat', 'of', 'the', 'prev', 'item', '.', 'The', 'pipe', 'character', 'indicat', 'a', 'choice', 'between', 'the', 'material', 'on', 'it', 'left', 'or', 'it', 'right', '.', 'Parenthes', 'indicate', 'the', 'scope', 'of', 'an', 'operator', ':', 'they', 'can', 'be', 'us', 'together', 'with', 'the', 'pipe', '(', 'or', 'disjunction', ')', 'symbol', 'like', 'thi', ':', '<', '<', 'w', '(', 'i|e|ai|oo', ')', 't', '>', '>', ',', 'match', 'wit', ',', 'wet', ',', 'wait', ',', 'and', 'woot', '.', 'It', 'i', 'instruct', 'to', 'see', 'what', 'happen', 'when', 'you', 'omit', 'the', 'parenthes', 'from', 'the', 'last', 'expression', 'above', '.']\n"
     ]
    }
   ],
   "source": [
    "#tokenize every letter and then remove all the suffixes based on the stem function.\n",
    "\n",
    "import re \n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "\n",
    "def stem(word):\n",
    "    regexp =r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$'\n",
    "    stem,suffix = re.findall(regexp,word)[0]\n",
    "    return stem\n",
    "\n",
    "sampleText= \"\"\"You probably worked out that a backlash means that\n",
    " the following character is deprived of its special powers\n",
    " and must literally match a specific character in they word.\n",
    " Thus, while . is special, \\. only matches a period. The \n",
    " braced expressions like {3,5}, specify the number of repeats\n",
    " of the previous item. The pipe character indicates a choice \n",
    " between the material on its left or its right. Parentheses \n",
    " indicate the scope of an operator: they can be used together \n",
    " with the pipe (or disjunction) symbol like this: <<w(i|e|ai|oo)t>>, \n",
    " matching wit, wet, wait, and woot. It is instructive to see what \n",
    " happens when you omit the parentheses from the last expression \n",
    " above.\"\"\"\n",
    "\n",
    "tokens = word_tokenize(sampleText)\n",
    "\n",
    "print([stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2009', '12', '31']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def stemDate(word):\n",
    "    result = [n for n in re.findall('[0-9]+','2009-12-31')]\n",
    "    return result\n",
    "    \n",
    "print(stemDate('2008-12-31'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['do', 'n\"t']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def stemDate(word):\n",
    "    result = [n for n in re.findall('n\"t|\\w+[o]',word)]\n",
    "    return result\n",
    "    \n",
    "print(stemDate('don\"t'))\n",
    "#'n\"t|\\w' does not work because of the \\w+ takes everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$310|5w33t!()!@#)!@at3555\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def convertText(word):\n",
    "    splitted = [n for n in re.findall('ate|.',word)]\n",
    "    result = []\n",
    "    count = 0\n",
    "    for x in splitted:\n",
    "        x = re.sub('s','aaaa',x)\n",
    "        if x == 'i':\n",
    "            result.append('1')\n",
    "        elif x == 'o':\n",
    "            result.append('0')\n",
    "        elif x == 'l':\n",
    "            result.append('|')\n",
    "        elif x == '.':\n",
    "            result.append('5w33t!')\n",
    "        elif x == 'ate':\n",
    "            result.append('8')\n",
    "        elif x == 's':\n",
    "            if count == 0:\n",
    "                result.append('$')\n",
    "            else:\n",
    "                result.append('5')\n",
    "        else:\n",
    "            result.append(x)\n",
    "        count = count + 1\n",
    "    return result\n",
    "\n",
    "def substituteText(word):\n",
    "    word = re.sub('^s','$',word)\n",
    "    word = re.sub('s','5',word)\n",
    "    word = re.sub('e','3',word)\n",
    "    word = re.sub('i','1',word)\n",
    "    word = re.sub('o','0',word)\n",
    "    word = re.sub('l','|',word)\n",
    "    word = re.sub('[.]','5w33t!',word)\n",
    "    word = re.sub('ate','8',word)\n",
    "    return word\n",
    "\n",
    "print(substituteText('sEi0l.()!@#)!@atesss'.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
