{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I will to show how to use bayes on multi-class classification/discrimination\n",
    "\n",
    "import class sklearn.naive_bayes.MultinomialNB for Multinomial logistic regression (logistic regression of multi-class)\n",
    "\n",
    "But if you want to classify binary/boolean class, it is better to use BernoulliNB "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use also compare accuracy for using BOW, TF-IDF, and HASHING for vectorizing technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ali/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# to get f1 score\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "import re\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some function to help us for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clear string\n",
    "def clearstring(string):\n",
    "    string = re.sub('[^A-Za-z0-9 ]+', '', string)\n",
    "    string = string.split(' ')\n",
    "    string = filter(None, string)\n",
    "    string = [y.strip() for y in string]\n",
    "    string = ' '.join(string)\n",
    "    return string\n",
    "\n",
    "# because of sklean.datasets read a document as a single element\n",
    "# so we want to split based on new line\n",
    "def separate_dataset(trainset):\n",
    "    datastring = []\n",
    "    datatarget = []\n",
    "    for i in range(len(trainset.data)):\n",
    "        data_ = trainset.data[i].split('\\n')\n",
    "        # python3, if python2, just remove list()\n",
    "        data_ = list(filter(None, data_))\n",
    "        for n in range(len(data_)):\n",
    "            data_[n] = clearstring(data_[n])\n",
    "        datastring += data_\n",
    "        for n in range(len(data_)):\n",
    "            datatarget.append(trainset.target[i])\n",
    "    return datastring, datatarget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I included 6 classes in local/\n",
    "1. adidas (wear)\n",
    "2. apple (electronic)\n",
    "3. hungry (status)\n",
    "4. kerajaan (government related)\n",
    "5. nike (wear)\n",
    "6. pembangkang (opposition related)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of Classes: ['adidas', 'apple', 'hungry', 'kerajaan', 'nike', 'pembangkang']\n",
      "# of Samples: 25292\n",
      "# of Samples: 25292\n"
     ]
    }
   ],
   "source": [
    "# you can change any encoding type\n",
    "trainset = sklearn.datasets.load_files(container_path = 'local', encoding = 'UTF-8')\n",
    "trainset.data, trainset.target = separate_dataset(trainset)\n",
    "print (\"List of Classes: %s\" %trainset.target_names)\n",
    "print (\"# of Samples: %s\" %len(trainset.data))\n",
    "print (\"# of Samples: %s\" %len(trainset.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change n to see different samples from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Why did the sandvich cross the road\n",
      "Class: hungry\n"
     ]
    }
   ],
   "source": [
    "n=10002\n",
    "print(\"Sentence: %s\" %trainset.data[n])\n",
    "print(\"Class: %s\" %trainset.target_names[trainset.target[n]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we got 25292 of strings, and 6 classes\n",
    "\n",
    "It is time to change it into vector representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag-of-word\n",
    "bow = CountVectorizer().fit_transform(trainset.data)\n",
    "\n",
    "#tf-idf, must get from BOW first\n",
    "tfidf = TfidfTransformer().fit_transform(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed Naive Bayes using BOW\n",
    "\n",
    "but split it first into train-set (80% of our data-set), and validation-set (20% of our data-set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy validation set: 0.851749357581\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     adidas       0.89      0.76      0.82       323\n",
      "      apple       0.78      0.91      0.84       460\n",
      "     hungry       0.86      0.95      0.90      1055\n",
      "   kerajaan       0.86      0.83      0.84      1382\n",
      "       nike       0.90      0.82      0.86       337\n",
      "pembangkang       0.85      0.82      0.83      1502\n",
      "\n",
      "avg / total       0.85      0.85      0.85      5059\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_Y, test_Y = train_test_split(bow, trainset.target, test_size = 0.2)\n",
    "\n",
    "bayes_multinomial = MultinomialNB().fit(train_X, train_Y)\n",
    "predicted = bayes_multinomial.predict(test_X)\n",
    "print('accuracy validation set: %s' %np.mean(predicted == test_Y))\n",
    "\n",
    "# print scores\n",
    "print(metrics.classification_report(test_Y, predicted, target_names = trainset.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed Naive Bayes using TF-IDF\n",
    "\n",
    "but split it first into train-set (80% of our data-set), and validation-set (20% of our data-set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy validation set: 0.801937141728\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     adidas       0.96      0.57      0.71       311\n",
      "      apple       0.98      0.58      0.73       477\n",
      "     hungry       0.79      0.91      0.85      1010\n",
      "   kerajaan       0.86      0.83      0.85      1408\n",
      "       nike       0.94      0.56      0.70       314\n",
      "pembangkang       0.71      0.87      0.78      1539\n",
      "\n",
      "avg / total       0.82      0.80      0.80      5059\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_Y, test_Y = train_test_split(tfidf, trainset.target, test_size = 0.2)\n",
    "\n",
    "bayes_multinomial = MultinomialNB().fit(train_X, train_Y)\n",
    "predicted = bayes_multinomial.predict(test_X)\n",
    "print('accuracy validation set: %s' %np.mean(predicted == test_Y))\n",
    "\n",
    "# print scores\n",
    "print(metrics.classification_report(test_Y, predicted, target_names = trainset.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed Naive Bayes using hashing\n",
    "\n",
    "but split it first into train-set (80% of our data-set), and validation-set (20% of our data-set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy validation set: 0.776438031231\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     adidas       0.98      0.53      0.69       348\n",
      "      apple       1.00      0.47      0.64       483\n",
      "     hungry       0.91      0.89      0.90      1076\n",
      "   kerajaan       0.85      0.79      0.82      1360\n",
      "       nike       0.96      0.54      0.69       321\n",
      "pembangkang       0.61      0.89      0.72      1471\n",
      "\n",
      "avg / total       0.82      0.78      0.77      5059\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_Y, test_Y = train_test_split(hashing, trainset.target, test_size = 0.2)\n",
    "\n",
    "bayes_multinomial = MultinomialNB().fit(train_X, train_Y)\n",
    "predicted = bayes_multinomial.predict(test_X)\n",
    "print('accuracy validation set: %s' %np.mean(predicted == test_Y))\n",
    "\n",
    "# print scores\n",
    "print(metrics.classification_report(test_Y, predicted, target_names = trainset.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
